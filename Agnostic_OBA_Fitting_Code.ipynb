{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdEZrodGYcK181ss9/O+xF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OJB-Quantum/Onri-Bezier-Approximation/blob/main/Agnostic_OBA_Fitting_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE0Q2FRUUh03"
      },
      "outputs": [],
      "source": [
        "# OBA Fit Overlay — Agnostic Post-Plot Script\n",
        "# Authored by Onri Jay Benally (2025)\n",
        "# -------------------------------------------------------------------\n",
        "# Paste this AFTER you've drawn any Matplotlib line plots.\n",
        "# It will read Line2D objects from the current figure/axes,\n",
        "# compute a high-resolution OBA fit per contiguous segment,\n",
        "# and overlay dashed curves (and optional anchors).\n",
        "#\n",
        "# Controls are grouped below. Defaults are safe.\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams.setdefault(\"figure.dpi\", 250)\n",
        "\n",
        "# ---- NumPy 2.0 deprecation shim (trapz → trapezoid) ----\n",
        "try:\n",
        "    _trapz = np.trapezoid   # NumPy >= 2.0\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz       # older NumPy\n",
        "\n",
        "# =========================\n",
        "# 0) User controls\n",
        "# =========================\n",
        "# Scope\n",
        "PROCESS_ALL_AXES: bool = False   # False: only current axes (plt.gca()); True: all axes in current figure\n",
        "FIT_ONLY_VISIBLE: bool = True    # skip hidden lines\n",
        "MIN_POINTS_PER_SEG: int = 5      # skip segments shorter than this\n",
        "\n",
        "# Which lines to include/exclude by label (exact match or substring)\n",
        "INCLUDE_SUBSTRINGS = []          # e.g., [\"Sweep up\", \"mydata\"]\n",
        "EXCLUDE_SUBSTRINGS = [\"OBA fit\", \"OBA_anchors\"]  # avoid refitting overlays\n",
        "\n",
        "# OBA clustering & evaluation\n",
        "CLUSTER_PERCENTILE: float = 30       # 0–100; higher → cluster only at very sharpest bends\n",
        "CANDIDATE_OVERSAMPLE: int = 12       # try 40–80 for ultra-tight tracking on smooth data\n",
        "MAX_ANCHORS_PER_SEG: int = 400       # protective cap\n",
        "DENSIFY_ITERS: int = 28              # curvature-mass insertions\n",
        "DENSIFY_TOP_FRAC: float = 0.95       # encourage anchors into top-curvature zones\n",
        "PACKING_SCALE: float = 0.25          # scales exclusion radius (smaller → denser anchors)\n",
        "TANGENT_SOURCE: str = \"candidate\"    # \"candidate\" | \"centripetal\" | \"monotone\" (monotone uses x; best when x is monotone)\n",
        "\n",
        "# Optional tetration-like hybrid growth kernel (OFF by default)\n",
        "TETRA_ENABLED: bool  = False         # set True to activate\n",
        "TETRA_MODE: str      = \"log\"         # \"log\" (stable), \"direct\", or \"series\"\n",
        "TETRA_HEIGHT: int    = 3             # height for iterated exponentiation\n",
        "TETRA_SERIES_TERMS: int = 5          # terms for \"series\" mode\n",
        "TETRA_SCALE: float   = 0.02          # amplitude as fraction of the fitted curve’s data range\n",
        "TETRA_AXIS: str      = \"y\"           # \"y\" or \"x\"\n",
        "\n",
        "# Rendering\n",
        "SHOW_ANCHORS: bool = True\n",
        "ANCHOR_SIZE: float = 12.0\n",
        "FIT_LINESTYLE: str = \"--\"\n",
        "FIT_ALPHA: float = 1.0\n",
        "ANCHOR_ALPHA: float = 1.0\n",
        "\n",
        "# Hygiene (re-running the cell)\n",
        "REMOVE_PREVIOUS_OVERLAYS: bool = True\n",
        "OVERLAY_GID = \"oba_fit_overlay\"\n",
        "ANCHOR_GID  = \"oba_fit_anchors\"\n",
        "\n",
        "# =========================\n",
        "# 1) Core helpers\n",
        "# =========================\n",
        "def _smooth_1d(z: np.ndarray, win: int) -> np.ndarray:\n",
        "    if win <= 1:\n",
        "        return z.copy()\n",
        "    k = np.ones(int(win), float) / float(win)\n",
        "    pad = int(win) // 2\n",
        "    zp = np.pad(z, (pad, pad), mode=\"reflect\")\n",
        "    return np.convolve(zp, k, mode=\"valid\")\n",
        "\n",
        "def _parametric_curvature(sv: np.ndarray, xv: np.ndarray, yv: np.ndarray) -> np.ndarray:\n",
        "    x_s = np.gradient(xv, sv, edge_order=1)\n",
        "    y_s = np.gradient(yv, sv, edge_order=1)\n",
        "    x_ss = np.gradient(x_s, sv, edge_order=1)\n",
        "    y_ss = np.gradient(y_s, sv, edge_order=1)\n",
        "    num = np.abs(x_s * y_ss - y_s * x_ss)\n",
        "    den = (x_s**2 + y_s**2)**1.5 + 1e-12\n",
        "    return num / den\n",
        "\n",
        "def _hermite_to_bezier(Pi, Ti, Pj, Tj, si, sj, mseg: int):\n",
        "    h = sj - si\n",
        "    c0, c3 = Pi, Pj\n",
        "    c1 = Pi + (Ti * h / 3.0)\n",
        "    c2 = Pj - (Tj * h / 3.0)\n",
        "    t = np.linspace(0.0, 1.0, int(mseg))\n",
        "    b = ((1 - t)[:, None] ** 3) * c0 + (3 * (1 - t)[:, None] ** 2 * t[:, None]) * c1 \\\n",
        "        + (3 * (1 - t)[:, None] * t[:, None] ** 2) * c2 + (t[:, None] ** 3) * c3\n",
        "    return b\n",
        "\n",
        "def _tangents_centripetal(P: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
        "    n = len(P)\n",
        "    T = np.zeros_like(P, float)\n",
        "    t = np.zeros(n, float)\n",
        "    for i in range(1, n):\n",
        "        t[i] = t[i-1] + np.linalg.norm(P[i] - P[i-1])**alpha\n",
        "    for i in range(n):\n",
        "        if i == 0:\n",
        "            dt = t[1] - t[0] if t[1] > t[0] else 1.0\n",
        "            T[i] = (P[1] - P[0]) / dt\n",
        "        elif i == n - 1:\n",
        "            dt = t[-1] - t[-2] if t[-1] > t[-2] else 1.0\n",
        "            T[i] = (P[-1] - P[-2]) / dt\n",
        "        else:\n",
        "            dt = t[i+1] - t[i-1] if t[i+1] > t[i-1] else 1.0\n",
        "            T[i] = (P[i+1] - P[i-1]) / dt\n",
        "    return T\n",
        "\n",
        "def _pchip_slopes(s: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    s = np.asarray(s, float); y = np.asarray(y, float)\n",
        "    n = len(s)\n",
        "    m = np.zeros(n, float)\n",
        "    ds = np.diff(s); dy = np.diff(y)\n",
        "    d = dy / (ds + 1e-15)\n",
        "    m[0] = d[0]; m[-1] = d[-1]\n",
        "    for i in range(1, n-1):\n",
        "        if d[i-1] * d[i] <= 0:\n",
        "            m[i] = 0.0\n",
        "        else:\n",
        "            w1 = 2*ds[i] + ds[i-1]\n",
        "            w2 = ds[i] + 2*ds[i-1]\n",
        "            m[i] = (w1 + w2) / (w1/(d[i-1]+1e-15) + w2/(d[i]+1e-15))\n",
        "    return m\n",
        "\n",
        "def _pchip_eval(s: np.ndarray, y: np.ndarray, m: np.ndarray, s_eval: np.ndarray) -> np.ndarray:\n",
        "    s = np.asarray(s, float); y = np.asarray(y, float); m = np.asarray(m, float)\n",
        "    s_eval = np.asarray(s_eval, float)\n",
        "    idx = np.searchsorted(s, s_eval, side=\"right\") - 1\n",
        "    idx = np.clip(idx, 0, len(s)-2)\n",
        "    s0 = s[idx]; s1 = s[idx+1]\n",
        "    y0 = y[idx]; y1 = y[idx+1]\n",
        "    m0 = m[idx]; m1 = m[idx+1]\n",
        "    h = (s_eval - s0) / (s1 - s0 + 1e-15)\n",
        "    h2 = h*h; h3 = h2*h\n",
        "    H00 = 2*h3 - 3*h2 + 1\n",
        "    H10 = h3 - 2*h2 + h\n",
        "    H01 = -2*h3 + 3*h2\n",
        "    H11 = h3 - h2\n",
        "    return H00*y0 + H10*(s1 - s0)*m0 + H01*y1 + H11*(s1 - s0)*m1\n",
        "\n",
        "def _dense_candidate_curve_generic(x: np.ndarray, y: np.ndarray, oversample: int = 40):\n",
        "    \"\"\"Centripetal-PCHIP in arclength for *generic* (possibly non-monotone) x,y curves.\"\"\"\n",
        "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
        "    # Remove NaNs up front (segments will be pre-split; this is a safety net)\n",
        "    good = np.isfinite(x) & np.isfinite(y)\n",
        "    x = x[good]; y = y[good]\n",
        "    if len(x) < 2:\n",
        "        return np.array([0.0]), x.copy(), y.copy()\n",
        "    dx, dy = np.diff(x), np.diff(y)\n",
        "    ds = np.hypot(dx, dy)\n",
        "    s = np.concatenate([[0.0], np.cumsum(ds)])\n",
        "    if s[-1] == 0:\n",
        "        s = np.linspace(0, 1, len(x))\n",
        "    else:\n",
        "        s = s / s[-1]\n",
        "    n_f = max(8*len(s), int(len(s) * max(4, int(oversample))))\n",
        "    s_f = np.linspace(0.0, 1.0, int(n_f))\n",
        "\n",
        "    # PCHIP on x(s) and y(s) independently (parametric Hermite)\n",
        "    mx = _pchip_slopes(s, x); my = _pchip_slopes(s, y)\n",
        "    x_f = _pchip_eval(s, x, mx, s_f)\n",
        "    y_f = _pchip_eval(s, y, my, s_f)\n",
        "    return s_f, x_f, y_f\n",
        "\n",
        "def _tangents_from_candidate(s_f: np.ndarray, x_f: np.ndarray, y_f: np.ndarray, keep_idx: np.ndarray) -> np.ndarray:\n",
        "    dx_ds = np.gradient(x_f, s_f, edge_order=1)\n",
        "    dy_ds = np.gradient(y_f, s_f, edge_order=1)\n",
        "    return np.stack([dx_ds[keep_idx], dy_ds[keep_idx]], axis=1)\n",
        "\n",
        "# ---- Tetration-like hybrid growth kernel (optional) ----\n",
        "def _tetration_unit(z: np.ndarray, height: int) -> np.ndarray:\n",
        "    z = np.clip(z, 0.0, 1.0) + 1e-15\n",
        "    out = z.copy()\n",
        "    h = int(max(1, height))\n",
        "    for _ in range(h - 1):\n",
        "        out = np.power(z, np.clip(out, 0.0, 1.0))\n",
        "    return out\n",
        "\n",
        "def _tetration_series(z: np.ndarray, terms: int) -> np.ndarray:\n",
        "    s = np.zeros_like(z)\n",
        "    T = int(max(1, terms))\n",
        "    for k in range(1, T + 1):\n",
        "        s += z**k / math.factorial(k)\n",
        "    return s\n",
        "\n",
        "def _hybrid_bump(n_points: int, mode: str, height: int, series_terms: int) -> np.ndarray:\n",
        "    sc = np.linspace(0.0, 1.0, int(max(2, n_points)))\n",
        "    if mode == \"direct\":\n",
        "        b = _tetration_unit(sc, height)\n",
        "    elif mode == \"series\":\n",
        "        b = _tetration_series(sc, series_terms)\n",
        "    else:  # \"log\" (stable default)\n",
        "        b = np.log(_tetration_unit(sc, height) + 1.0)\n",
        "    b = b - b.mean()\n",
        "    w = 0.5 - 0.5*np.cos(2*np.pi*sc)  # Hann window → pinned endpoints\n",
        "    return b * w\n",
        "\n",
        "def _map_percentile_to_hparams(p: float):\n",
        "    a = float(np.clip(p, 0.0, 100.0)) / 100.0\n",
        "    return dict(\n",
        "        r_base=float(np.interp(a, [0, 1], [0.01, 0.08])),\n",
        "        r_min_floor=float(np.interp(a, [0, 1], [1e-7, 5e-7])),\n",
        "        r_shrink_max=float(np.interp(a, [0, 1], [0.97, 0.999])),\n",
        "        r_power=float(np.interp(a, [0, 1], [4.0, 8.0])),\n",
        "        smooth_window=int(round(np.interp(a, [0, 1], [7, 13]))),\n",
        "        num_seg_per_bezier=int(round(np.interp(a, [0, 1], [260, 360]))),\n",
        "    )\n",
        "\n",
        "def _oba_fit_highres_follow_generic(\n",
        "    x: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    cluster_percentile: float = 30.0,\n",
        "    oversample: int = 12,\n",
        "    max_anchors: int = 400,\n",
        "    densify_iters: int = 28,\n",
        "    densify_top_frac: float = 0.95,\n",
        "    packing_scale: float = 0.25,\n",
        "    tangent_source: str = \"candidate\",\n",
        "    tetra_enabled: bool = False,\n",
        "    tetra_mode: str = \"log\",\n",
        "    tetra_height: int = 3,\n",
        "    tetra_series_terms: int = 5,\n",
        "    tetra_scale: float = 0.02,\n",
        "    tetra_axis: str = \"y\",\n",
        "):\n",
        "    # Dense candidate parametric curve\n",
        "    s_f, x_f, y_f = _dense_candidate_curve_generic(x, y, oversample=max(4, int(oversample)))\n",
        "    hp = _map_percentile_to_hparams(cluster_percentile)\n",
        "\n",
        "    # Curvature weight\n",
        "    kappa = _smooth_1d(_parametric_curvature(s_f, x_f, y_f), hp[\"smooth_window\"])\n",
        "    kappa = np.maximum(kappa, 0.0)\n",
        "    thr = np.percentile(kappa, float(cluster_percentile))\n",
        "    kmax = float(kappa.max()) if kappa.size else 1.0\n",
        "    w = np.clip((kappa - thr) / (kmax - thr + 1e-15), 0.0, 1.0)\n",
        "\n",
        "    # Pass 1: variable-radius greedy selection\n",
        "    r_local = hp[\"r_base\"] * (1.0 - hp[\"r_shrink_max\"] * (w ** hp[\"r_power\"]))\n",
        "    r_local = np.maximum(r_local, hp[\"r_min_floor\"]) * float(max(1e-6, packing_scale))\n",
        "    order = np.argsort(-w).astype(int)\n",
        "    order = order[(order >= 0) & (order < len(s_f))]\n",
        "\n",
        "    keep = [0, len(s_f)-1]\n",
        "    kept = np.zeros(len(s_f), bool); kept[0] = kept[-1] = True\n",
        "\n",
        "    def _too_close(i: int) -> bool:\n",
        "        for j in np.where(kept)[0]:\n",
        "            if abs(s_f[i] - s_f[j]) < min(r_local[i], r_local[j]):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    for i in order:\n",
        "        if kept[i]:\n",
        "            continue\n",
        "        if not _too_close(i):\n",
        "            keep.append(i); kept[i] = True\n",
        "        if len(keep) >= max_anchors:\n",
        "            break\n",
        "\n",
        "    keep.sort()\n",
        "    keep = np.array(keep, int)\n",
        "\n",
        "    # Pass 2: densify where curvature mass is largest\n",
        "    target_top = int(densify_top_frac * len(keep))\n",
        "    for _ in range(int(densify_iters)):\n",
        "        thr95 = np.percentile(kappa, 95.0)\n",
        "        n_top = np.count_nonzero(kappa[keep] >= thr95)\n",
        "        if n_top >= target_top or len(keep) >= max_anchors:\n",
        "            break\n",
        "        best_gain = 0.0; best_pos = None; best_insert = None\n",
        "        for a_idx in range(len(keep)-1):\n",
        "            i, j = keep[a_idx], keep[a_idx+1]\n",
        "            if j <= i + 1:\n",
        "                continue\n",
        "            window = slice(i+1, j)\n",
        "            mass = _trapz(w[window], s_f[window])\n",
        "            if mass > best_gain:\n",
        "                loc = int(np.argmax(kappa[window])) + (i+1)\n",
        "                best_gain = mass; best_pos = loc; best_insert = a_idx + 1\n",
        "        if best_pos is None:\n",
        "            break\n",
        "        pos = int(best_pos)\n",
        "        if pos < 0 or pos >= len(s_f):\n",
        "            continue\n",
        "        if not _too_close(pos):\n",
        "            keep = np.insert(keep, best_insert, pos)\n",
        "\n",
        "    # Build anchors & tangents\n",
        "    s_a = s_f[keep]\n",
        "    P_a = np.stack([x_f[keep], y_f[keep]], axis=1)\n",
        "\n",
        "    if tangent_source == \"candidate\":\n",
        "        T = _tangents_from_candidate(s_f, x_f, y_f, keep)\n",
        "    elif tangent_source == \"centripetal\":\n",
        "        T = _tangents_centripetal(P_a, alpha=0.5)\n",
        "    elif tangent_source == \"monotone\":\n",
        "        # Monotone variant is less meaningful for generic curves; fallback to candidate if unstable\n",
        "        try:\n",
        "            # Map monotone in s (always monotone), approximate with PCHIP slope on y vs s and x vs s\n",
        "            dx_ds = np.gradient(x_f, s_f, edge_order=1)\n",
        "            dy_ds = np.gradient(y_f, s_f, edge_order=1)\n",
        "            T = np.stack([dx_ds[keep], dy_ds[keep]], axis=1)\n",
        "        except Exception:\n",
        "            T = _tangents_centripetal(P_a, alpha=0.5)\n",
        "    else:\n",
        "        T = _tangents_centripetal(P_a, alpha=0.5)\n",
        "\n",
        "    # Composite Bézier evaluation\n",
        "    seg_pts = []\n",
        "    for i in range(len(s_a)-1):\n",
        "        seg = _hermite_to_bezier(P_a[i], T[i], P_a[i+1], T[i+1],\n",
        "                                 s_a[i], s_a[i+1], hp[\"num_seg_per_bezier\"])\n",
        "        if i > 0: seg = seg[1:]  # avoid duplicate joints\n",
        "        seg_pts.append(seg)\n",
        "    seg_pts = np.vstack(seg_pts) if seg_pts else P_a.copy()\n",
        "\n",
        "    # Optional tetration-like bump\n",
        "    if tetra_enabled and len(seg_pts) > 2:\n",
        "        bump = _hybrid_bump(len(seg_pts), tetra_mode, tetra_height, tetra_series_terms)\n",
        "        if tetra_axis.lower() == \"y\":\n",
        "            amp = float(tetra_scale) * max(1e-15, np.ptp(y_f))\n",
        "            seg_pts[:, 1] = seg_pts[:, 1] + amp * bump\n",
        "        else:\n",
        "            amp = float(tetra_scale) * max(1e-15, np.ptp(x_f))\n",
        "            seg_pts[:, 0] = seg_pts[:, 0] + amp * bump\n",
        "\n",
        "    return seg_pts[:,0], seg_pts[:,1], P_a\n",
        "\n",
        "# =========================\n",
        "# 2) Axes crawler + overlay\n",
        "# =========================\n",
        "def _split_nan_segments(x: np.ndarray, y: np.ndarray):\n",
        "    \"\"\"Yield contiguous (x,y) segments with finite data.\"\"\"\n",
        "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
        "    good = np.isfinite(x) & np.isfinite(y)\n",
        "    if not np.any(good):\n",
        "        return\n",
        "    idx = np.where(~good)[0]\n",
        "    # segment boundaries across NaNs\n",
        "    starts = np.r_[0, idx + 1]\n",
        "    stops  = np.r_[idx, len(x) - 1]\n",
        "    # filter valid ranges\n",
        "    mask = starts <= stops\n",
        "    starts = starts[mask]; stops = stops[mask]\n",
        "    # merge consecutive NaNs collapse\n",
        "    last_stop = -1\n",
        "    for s, e in zip(starts, stops):\n",
        "        if s <= last_stop:\n",
        "            continue\n",
        "        # extend s forward to first good\n",
        "        while s <= e and not (np.isfinite(x[s]) and np.isfinite(y[s])):\n",
        "            s += 1\n",
        "        # shrink e backward to last good\n",
        "        while e >= s and not (np.isfinite(x[e]) and np.isfinite(y[e])):\n",
        "            e -= 1\n",
        "        if e - s + 1 >= MIN_POINTS_PER_SEG:\n",
        "            yield x[s:e+1], y[s:e+1]\n",
        "        last_stop = e\n",
        "\n",
        "def _label_included(label: str) -> bool:\n",
        "    if any(sub in label for sub in EXCLUDE_SUBSTRINGS):\n",
        "        return False\n",
        "    if INCLUDE_SUBSTRINGS:\n",
        "        return any(sub in label for sub in INCLUDE_SUBSTRINGS)\n",
        "    return True  # include by default\n",
        "\n",
        "def apply_oba_to_axes(ax: plt.Axes):\n",
        "    if REMOVE_PREVIOUS_OVERLAYS:\n",
        "        old = [ln for ln in ax.get_lines() if ln.get_gid() in (OVERLAY_GID, ANCHOR_GID)]\n",
        "        for ln in old:\n",
        "            try:\n",
        "                ln.remove()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    for line in list(ax.get_lines()):\n",
        "        label = line.get_label() or \"\"\n",
        "        if line.get_gid() in (OVERLAY_GID, ANCHOR_GID):\n",
        "            continue\n",
        "        if not _label_included(label):\n",
        "            continue\n",
        "        if FIT_ONLY_VISIBLE and (not line.get_visible()):\n",
        "            continue\n",
        "\n",
        "        xdata = np.asarray(line.get_xdata(), float)\n",
        "        ydata = np.asarray(line.get_ydata(), float)\n",
        "        if len(xdata) < MIN_POINTS_PER_SEG:\n",
        "            continue\n",
        "\n",
        "        color = line.get_color()\n",
        "        lw_fit = max(0.8, 0.9 * line.get_linewidth())\n",
        "\n",
        "        for xs, ys in _split_nan_segments(xdata, ydata):\n",
        "            if len(xs) < MIN_POINTS_PER_SEG:\n",
        "                continue\n",
        "            bx, by, anchors = _oba_fit_highres_follow_generic(\n",
        "                xs, ys,\n",
        "                cluster_percentile=CLUSTER_PERCENTILE,\n",
        "                oversample=CANDIDATE_OVERSAMPLE,\n",
        "                max_anchors=MAX_ANCHORS_PER_SEG,\n",
        "                densify_iters=DENSIFY_ITERS,\n",
        "                densify_top_frac=DENSIFY_TOP_FRAC,\n",
        "                packing_scale=PACKING_SCALE,\n",
        "                tangent_source=TANGENT_SOURCE,\n",
        "                tetra_enabled=TETRA_ENABLED,\n",
        "                tetra_mode=TETRA_MODE,\n",
        "                tetra_height=TETRA_HEIGHT,\n",
        "                tetra_series_terms=TETRA_SERIES_TERMS,\n",
        "                tetra_scale=TETRA_SCALE,\n",
        "                tetra_axis=TETRA_AXIS,\n",
        "            )\n",
        "\n",
        "            # Overlay the fitted curve\n",
        "            ax.plot(\n",
        "                bx, by,\n",
        "                FIT_LINESTYLE,\n",
        "                color=color,\n",
        "                alpha=FIT_ALPHA,\n",
        "                linewidth=lw_fit,\n",
        "                label=(label + \" (OBA fit)\") if label and label != \"_nolegend_\" else \"OBA fit\",\n",
        "                gid=OVERLAY_GID,\n",
        "                zorder=(line.get_zorder() + 0.1),\n",
        "            )\n",
        "\n",
        "            # Optional anchors\n",
        "            if SHOW_ANCHORS and len(anchors) > 0:\n",
        "                ax.scatter(\n",
        "                    anchors[:, 0], anchors[:, 1],\n",
        "                    s=ANCHOR_SIZE, marker=\"o\",\n",
        "                    color=color, alpha=ANCHOR_ALPHA,\n",
        "                    label=\"OBA anchors\" if label == \"\" or label == \"_nolegend_\" else f\"{label} (OBA anchors)\",\n",
        "                    gid=ANCHOR_GID,\n",
        "                    zorder=(line.get_zorder() + 0.2),\n",
        "                )\n",
        "\n",
        "    # Keep legends sane: deduplicate labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    uniq = {}\n",
        "    for h, l in zip(handles, labels):\n",
        "        if l not in uniq:\n",
        "            uniq[l] = h\n",
        "    if uniq:\n",
        "        ax.legend(uniq.values(), uniq.keys(), ncol=2, fontsize=9)\n",
        "\n",
        "# =========================\n",
        "# 3) Run on current figure\n",
        "# =========================\n",
        "if PROCESS_ALL_AXES:\n",
        "    fig = plt.gcf()\n",
        "    for _ax in fig.axes:\n",
        "        apply_oba_to_axes(_ax)\n",
        "else:\n",
        "    apply_oba_to_axes(plt.gca())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.draw()"
      ]
    }
  ]
}